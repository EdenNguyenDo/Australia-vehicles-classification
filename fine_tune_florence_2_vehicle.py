# -*- coding: utf-8 -*-
"""fine-tune-florence-2-vehicle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-GAqpF8VsjM8XkVO7DBA8GLpgXGJOwxj

## Setup
"""
from server import DEVICE

# !pip install timm flash_attn einops;
# !pip install -q roboflow git+https://github.com/roboflow/supervision.git

"""Now we import the packages we'll need, including the `utils.py` module from the repository that we just cloned. 
This file provides misellaneous functionality to make it easier to work with Florence-2."""

# Commented out IPython magic to ensure Python compatibility.

from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import utils
import torch

# %matplotlib inline

"""Next we load the Florence-2 model and processor"""

CHECKPOINT = "microsoft/Florence-2-large"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)
processor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)

"""And then we set these models as constants for our `utils.py` module so that the functions can utilize them as global constants."""

utils.set_model_info(model, processor)

"""LOAD IMAGES"""

path = "dataset/train/frame2.png"
image = Image.open(path)
image_rgb = Image.open(path).convert("RGB")

"""#### Object detection

Object detection automatically detects the salient objects in an image. Florence-2 supports 3 levels of semantic granularity:
1. None (bounding boxes only)
2. Categorical labels
3. Descriptive labels
"""

tasks = [utils.TaskType.OBJECT_DETECTION]

for task in tasks:
  results = utils.run_example(task, image_rgb)
  print(task.value)
  utils.plot_bbox(results[task], image)

"""# FINE TUNING FLORENCE 2 AND TRAIN ON A CUSTOM DATASET"""

import os
import json
import torch

import numpy as np
import supervision as sv
from IPython.core.display import display, HTML
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AdamW,
    AutoModelForCausalLM,
    AutoProcessor,
    get_scheduler
)
from tqdm import tqdm
from typing import List, Dict, Any, Tuple, Generator
from peft import LoraConfig, get_peft_model
from PIL import Image
from roboflow import Roboflow




class JSONLDataset:
    """
    A dataset class for loading images and annotations from a JSONL file.

    Attributes:
        jsonl_file_path (str): Path to the JSONL file containing annotations.
        image_directory_path (str): Path to the directory containing images.
        entries (List[Dict[str, Any]]): Parsed list of annotation entries.
    """

    def __init__(self, jsonl_file_path: str, image_directory_path: str):
        """
        Initialize the dataset with a JSONL file and image directory.

        Args:
            jsonl_file_path (str): Path to the JSONL annotation file.
            image_directory_path (str): Path to the directory containing images.
        """
        self.jsonl_file_path = jsonl_file_path
        self.image_directory_path = image_directory_path
        self.entries = self._load_entries()

    def _load_entries(self) -> List[Dict[str, Any]]:
        """
        Parse the JSONL file to load annotation entries.

        Returns:
            List[Dict[str, Any]]: A list of parsed entries from the JSONL file.
        """
        entries = []
        with open(self.jsonl_file_path, 'r') as file:
            for line in file:
                data = json.loads(line)
                entries.append(data)
        return entries

    def __len__(self) -> int:
        """Return the number of entries in the dataset."""

        return len(self.entries)

    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:
        """
        Retrieve an image and its associated annotation.

        Args:
            idx (int): Index of the desired entry.

        Returns:
            Tuple[Image.Image, Dict[str, Any]]: The image and its annotation data.

        Raises:
            IndexError: If the index is out of range.
            FileNotFoundError: If the specified image file is not found.
        """
        if idx < 0 or idx >= len(self.entries):
            raise IndexError("Index out of range")

        entry = self.entries[idx]
        image_path = os.path.join(self.image_directory_path, entry['image'])
        try:
            image = Image.open(image_path)
            return (image, entry)
        except FileNotFoundError:
            raise FileNotFoundError(f"Image file {image_path} not found.")


class DetectionDataset(Dataset):
    """
    A PyTorch Dataset wrapper for handling detection datasets.

    Attributes:
        dataset (JSONLDataset): The underlying dataset object.
    """

    def __init__(self, jsonl_file_path: str, image_directory_path: str):
        """
        Initialize the detection dataset using a JSONL file and image directory.

        Args:
            jsonl_file_path (str): Path to the JSONL annotation file.
            image_directory_path (str): Path to the directory containing images.
        """
        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)

    def __len__(self):
        """Return the number of entries in the dataset."""
        return len(self.dataset)

    def __getitem__(self, idx):
        """
        Retrieve a dataset entry consisting of prefix, suffix, and image.

        Args:
            idx (int): Index of the desired entry.

        Returns:
            Tuple[str, str, Image.Image]: Prefix, suffix, and the associated image.
        """
        image, data = self.dataset[idx]
        prefix = data['prefix']
        suffix = data['suffix']
        return prefix, suffix, image

# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets

BATCH_SIZE = 6
NUM_WORKERS = 0

def collate_fn(batch):
    questions, answers, images = zip(*batch)
    images = [image.convert('RGB') for image in images]
    inputs = processor(text=list(questions), images=list(images), return_tensors="pt", padding=True).to(DEVICE)
    return inputs, answers

train_dataset = DetectionDataset(
    jsonl_file_path = "dataset/train/annotation.jsonl",
    image_directory_path = "dataset/train/"
)
val_dataset = DetectionDataset(
    jsonl_file_path = "dataset/valid/annotation.jsonl",
    image_directory_path = "dataset/valid/"
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)



"""### Setup LoRA Florence-2 model

LoRA is low-rank decomposition method to reduce the number of trainable parameters which speeds up finetuning large models and uses less memory.
"""

# @title Config Lora
config = LoraConfig(
    r=8,
    lora_alpha=8,
    target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "linear", "Conv2d", "lm_head", "fc2"],
    task_type="CAUSAL_LM",
    lora_dropout=0.05,
    bias="none",
    inference_mode=False,
    use_rslora=True,
    init_lora_weights="gaussian"
)

peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters()





# @title Define train loop

def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):
    optimizer = AdamW(model.parameters(), lr=lr)
    num_training_steps = epochs * len(train_loader)
    lr_scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps,
    )


    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for inputs, answers in tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{epochs}"):

            input_ids = inputs["input_ids"]
            pixel_values = inputs["pixel_values"]
            labels = processor.tokenizer(
                text=answers,
                return_tensors="pt",
                padding=True,
                return_token_type_ids=False
            ).input_ids.to(DEVICE)

            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
            loss = outputs.loss

            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        print(f"Average Training Loss: {avg_train_loss}")

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for inputs, answers in tqdm(val_loader, desc=f"Validation Epoch {epoch + 1}/{epochs}"):

                input_ids = inputs["input_ids"]
                pixel_values = inputs["pixel_values"]
                labels = processor.tokenizer(
                    text=answers,
                    return_tensors="pt",
                    padding=True,
                    return_token_type_ids=False
                ).input_ids.to(DEVICE)

                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
                loss = outputs.loss

                val_loss += loss.item()

            avg_val_loss = val_loss / len(val_loader)
            print(f"Average Validation Loss: {avg_val_loss}")

            render_inference_results(peft_model, val_loader.dataset, 6)

        output_dir = f"./model_checkpoints/epoch_{epoch+1}"
        os.makedirs(output_dir, exist_ok=True)
        model.save_pretrained(output_dir)
        processor.save_pretrained(output_dir)

# Commented out IPython magic to ensure Python compatibility.
# # @title Run train loop
# 
# %%time
# 
EPOCHS = 17
LR = 5e-6
# 
train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)


"""### Save the model"""

peft_model.save_pretrained("saved_model/ft-florence2-LORA")
processor.save_pretrained("saved_model/ft-florence2-LORA")


#
# !ls -la "/content/Australia-vehicles-classification/Australia-vehicles-classification/saved_model/florence2-lora"
#
# # Running inference
# !pip install inference
#
#
#
# import os
# from PIL import Image
# import json
# from google.colab import userdata
# import roboflow
#
# rf = Roboflow(api_key=userdata.get('ROBOFLOW_KEY'))
# project = rf.workspace("first-project-xpdzs").project("vehicle-detect-tksbg")
# version = project.version(3)
#
# version.deploy(model_type="florence-2-large", model_path="/content/Australia-vehicles-classification/Australia-vehicles-classification/saved_model/florence2-lora")
#
# from inference import get_model
#
# lora_model = get_model("vehicle-detect-tksbg/3", api_key=userdata.get('ROBOFLOW_KEY'))
#
# image = Image.open("Australia-vehicles-classification/dataset/test/frame7.png")
# response = lora_model.infer(image)
# print(response)
#
# import pdb
#
# !pip install -q colab_ssh --upgrade
# from colab_ssh import launch_ssh
# launch_ssh('your_ssh_key_password', '2ord4pJVqUZRzmVmwg4zfUDvq5s_2CGfao4dDhsC5wX8qyeXB')